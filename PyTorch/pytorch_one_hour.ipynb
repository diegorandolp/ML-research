{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PyTorch\n",
    "\n",
    "It is composed by:\n",
    "1. PyTorch tensors = numpy + GPU\n",
    "2. Autograd (automatic differentiation engine) to compute the gradients for tensor operations. Eg: backpropagation.\n",
    "3. Deep learning library that contains pre-trained models, loss functions, etc.\n",
    "\n",
    "We will go through every component"
   ],
   "id": "cdbac2ca0f0bd749"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PyTorch tensors",
   "id": "1600e76ca57e1f0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T16:12:18.173574Z",
     "start_time": "2025-07-19T16:12:17.035110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.__version__\n"
   ],
   "id": "ad5095b07147d2df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0+cu126'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T16:18:03.642335Z",
     "start_time": "2025-07-19T16:18:01.812121Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.is_available())",
   "id": "4688e7e9371a844d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T16:54:32.722241Z",
     "start_time": "2025-07-19T16:54:32.714614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 0D tensor (scalar)\n",
    "tensor0d = torch.tensor(1)\n",
    "print(tensor0d)\n",
    "# 1D tensor (vector)\n",
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "print(tensor1d)\n",
    "# 2D tensor (matrix)\n",
    "tensor2d = torch.tensor([[1, 2], [3, 4]])\n",
    "print(tensor2d)\n",
    "# 3D tensor\n",
    "tensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "print(tensor3d)"
   ],
   "id": "6a868c0348c33bad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T16:55:57.223879Z",
     "start_time": "2025-07-19T16:55:57.219773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# default 64-bit integer\n",
    "print(tensor1d.dtype)"
   ],
   "id": "f63bf91ada160602",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T16:56:50.357403Z",
     "start_time": "2025-07-19T16:56:50.351885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# default 32-bit precision\n",
    "floatvec = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(floatvec.dtype)"
   ],
   "id": "2db56ef9e8aa87c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T16:58:59.641769Z",
     "start_time": "2025-07-19T16:58:59.638965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# change type\n",
    "tensor1d_float = tensor1d.to(torch.float32)\n",
    "print(tensor1d_float.dtype)"
   ],
   "id": "cc467a3e3ded11fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:00:17.100318Z",
     "start_time": "2025-07-19T17:00:17.095530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# shape of a tensor\n",
    "print(tensor0d.shape)\n",
    "print(tensor1d.shape)\n",
    "print(tensor2d.shape)\n",
    "print(tensor3d.shape)"
   ],
   "id": "a4c42fdde2b99c18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:03:03.268705Z",
     "start_time": "2025-07-19T17:03:03.261999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reshape a tensor\n",
    "tensor2d.reshape(4, 1)"
   ],
   "id": "86c5c9614139f202",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:03:27.234043Z",
     "start_time": "2025-07-19T17:03:27.228310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reshape a tensor (common method)\n",
    "tensor2d.view(4, 1)"
   ],
   "id": "85712f799d16ae5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:03:48.136099Z",
     "start_time": "2025-07-19T17:03:48.123321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transpose\n",
    "tensor2d.T"
   ],
   "id": "147e923c57a12c00",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3],\n",
       "        [2, 4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:05:10.770053Z",
     "start_time": "2025-07-19T17:05:10.761116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# matmul 1\n",
    "tensor2d.matmul(tensor2d.T)"
   ],
   "id": "e77970fa87b3f102",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 11],\n",
       "        [11, 25]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:05:40.101808Z",
     "start_time": "2025-07-19T17:05:40.095706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# matmul 2\n",
    "tensor2d @ tensor2d.T"
   ],
   "id": "2b182f1ef034dbf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 11],\n",
       "        [11, 25]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PyTorch autograd engine",
   "id": "86885e521fa1e8a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:16:36.693355Z",
     "start_time": "2025-07-19T17:16:36.681447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Suppose we have a model with the weight w1 and th bias b,\n",
    "# to compute the gradients, pytorch computes a graph in the background\n",
    "# as shown in the following figure\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0]) # true label\n",
    "x1 = torch.tensor([1.1]) # input\n",
    "w1 = torch.tensor([2.2]) # weight\n",
    "b = torch.tensor([0.0]) # bias\n",
    "\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z) # predicted label\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "print(\"[a]\", a)\n",
    "print(\"[y]\", y)\n",
    "print(\"[loss]\", loss)"
   ],
   "id": "ee8a4f578fa27827",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[a] tensor([0.9183])\n",
      "[y] tensor([1.])\n",
      "[loss] tensor(0.0852)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following figure illustrates the graph of the above 'model'.\n",
    "\n",
    "As long as the final node, in this case `loss = L(a,y)` has the requires_grad attribute set to True, pytorch will build the graph to compute the gradients.\n",
    "\n",
    "The way pytorch compute the gradients is from right to left, called backpropagation, it starts from the output layer (loss) and goes backward to the input layer.\n",
    "\n",
    "In this way, pytorch computes the gradient of the loss respect to each parameter (weights and biases) to update these parameters during training.\n",
    "\n",
    "![pytorch_automatic_differentiation.png](./images/pytorch_automatic_differentiation.png)\n"
   ],
   "id": "cd09748e18b4c1b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:55:22.448040Z",
     "start_time": "2025-07-19T17:55:22.435653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# in the previous code the code pytorch didn't build the graph\n",
    "# because there were no terminal nodes with the requires_grad\n",
    "# as True. In this code, the graph is built\n",
    "\n",
    "# This is where the automatic differentiation engine is important,\n",
    "# given the graph, the engine can compute the gradients using the\n",
    "# function grad.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "# by default, the graph is deleted after the gradients are computed\n",
    "# we retain it to use it later\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b = grad(loss, b, retain_graph=True)\n",
    "\n",
    "print(grad_L_w1)\n",
    "print(grad_L_b)"
   ],
   "id": "dc0c694d69e7254f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:59:46.786473Z",
     "start_time": "2025-07-19T17:59:46.780413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# anyway, the common way to compute the gradients is using the\n",
    "# method backward, the results will be stored in the grad attribute\n",
    "loss.backward()\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ],
   "id": "2bbe6b911dfc8e4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PyTorch as a deep learning library",
   "id": "5b226567b114091a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Create a MLP with PyTorch, similar to the following image, this network will have:\n",
    "\n",
    "- 50 inputs for a data point with 50 features\n",
    "- 30 neurons in the 1st layer, resulting in:\n",
    "    - 50x30 weights to calculate\n",
    "    - 30 multilinear equations thus 30 biases\n",
    "- 20 neurons in the 2nd layer, resulting in:\n",
    "    - 30x20 weights to calculate\n",
    "    - 20 multilinear equations thus 20 biases\n",
    "- 3 outputs, resulting in:\n",
    "    - 20x3 weights to calculate\n",
    "    - 3 biases\n",
    "\n",
    "Counting all, it gives 2213 parameters to compute\n",
    "\n",
    "![mlp.png](./images/mlp.png)\n",
    "\n"
   ],
   "id": "c696af3f0009a3df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:08:47.739938Z",
     "start_time": "2025-07-19T22:08:47.735805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# our class inherits the Module subclass because it allows us to encapsulate\n",
    "# the layers and operations and track the model's parameters\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    # to define the network layers\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        # calls the Module class constructor\n",
    "        super().__init__()\n",
    "        # encapsulate all the layers\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # output layer of logits\n",
    "            torch.nn.Linear(20, num_outputs)\n",
    "        )\n",
    "    # to define how the input data passes through the network\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ],
   "id": "22ae964321e2479a",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:08:49.334351Z",
     "start_time": "2025-07-19T22:08:49.329624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model = NeuralNetwork(50, 3)\n",
    "print(model)"
   ],
   "id": "3326f4d7965725b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:08:50.761031Z",
     "start_time": "2025-07-19T22:08:50.756402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# same result calculated before\n",
    "num_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad\n",
    ")\n",
    "print('[Parameters]', num_params)"
   ],
   "id": "a7304f11b1afb88f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameters] 2213\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:08:52.187253Z",
     "start_time": "2025-07-19T22:08:52.180726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# As indicated above the trainable parameters have requires_grad\n",
    "# as True, it occurs in the Linear layers, for example, in the\n",
    "# first linear layer which was initialized with randoms:\n",
    "print(model.layers[0].weight)"
   ],
   "id": "67444dbda9ceed56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:08:53.663564Z",
     "start_time": "2025-07-19T22:08:53.658560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(model.layers[0].weight.shape)\n",
    "print(model.layers[0].bias.shape)"
   ],
   "id": "64e07160acd624b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 50])\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:08:57.709103Z",
     "start_time": "2025-07-19T22:08:57.702615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# forward pass without training\n",
    "\n",
    "# tensor of inputs\n",
    "X = torch.rand((1, 50))\n",
    "out = model.forward(X)\n",
    "print(out)"
   ],
   "id": "3582f475031c4475",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1670,  0.1001, -0.1219]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The previous result also gives us the last used function to compute in the graph, pytorch uses this information during backpropagation. It means, `mm` for matrix multiplication followed by `Add` for addition.\n",
    "\n",
    "When we use models only for inferent rather than training, we don't need the creation of the graph, in fact, it would be a waste of resources, so there is a better way in this case:"
   ],
   "id": "58cc4a421aee2655"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:23:08.461307Z",
     "start_time": "2025-07-19T22:23:08.456195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    out = model.forward(X)\n",
    "print(out)"
   ],
   "id": "e8e562dc4e340fec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1670,  0.1001, -0.1219]])\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Common practice**\n",
    "Create models that return `logits` as outputs without an activation function, the `logits` are Real numbers. This happens because pytorch combine the activation function and the loss for efficiency (use cancellation tricks), so the combined functions expect `logits` and output probabilities.\n",
    "Eg:\n",
    "- CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "- BCEWithLogitsLoss = Sigmoid + BCELoss"
   ],
   "id": "3e53073eb1968a5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:42:39.736197Z",
     "start_time": "2025-07-19T22:42:39.728095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# apply the activation function outside the creation of the model\n",
    "# this one in particular ensure all output values are positive and\n",
    "# sum 1\n",
    "with torch.no_grad():\n",
    "    out = torch.softmax(model(X), dim=1)\n",
    "print(out)"
   ],
   "id": "dceb0fb0288a52a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2983, 0.3896, 0.3121]])\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DataLoader\n",
    "\n",
    "Before we can train the neural network defined above, we have to create an efficient data loader.\n",
    "\n",
    "As shown in the figure, we first create a custom `Dataset` to implement methods to retrieve individual records, then we instantiate a training and a test `Dataset`. Both of them are passed to a `DataLoader` which will define how the data is shuffled and assembled into batches\n",
    "\n",
    "![dataloader.png](./images/dataloader.png)\n"
   ],
   "id": "3f33d3e7272ececa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T00:53:26.627010Z",
     "start_time": "2025-07-20T00:53:26.622472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To simulate the creation of a DataLoader, we can create a dataset\n",
    "# with train and test data\n",
    "x_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "x_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6]\n",
    "])\n",
    "\n",
    "y_test = torch.tensor([0, 1])"
   ],
   "id": "71dd569fe840f34f",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T00:59:39.144523Z",
     "start_time": "2025-07-20T00:59:39.139876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creation of the custom Dataset\n",
    "# IMPORTANT: the number of labels in the dataset can't exceed the\n",
    "# number of output nodes minus 1\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    # set up the attributes, it doesn't have to be tensors, it\n",
    "    # could be files, database connectors, etc\n",
    "    def __init__(self, x, y):\n",
    "        self.features = x\n",
    "        self.labels = y\n",
    "    # access to specific individual record\n",
    "    def __getitem__(self, index):\n",
    "        one_x = self.features[index]\n",
    "        one_y = self.labels[index]\n",
    "        return one_x, one_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "train_ds = ToyDataset(x_train, y_train)\n",
    "test_ds = ToyDataset(x_test, y_test)"
   ],
   "id": "4c7fec86e2a8cb3e",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T01:08:17.847484Z",
     "start_time": "2025-07-20T01:08:17.842465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate the DataLoader with the custom Datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2, # number of record retrieved in each iteration\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "id": "347e94f7285a3710",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T01:08:49.093740Z",
     "start_time": "2025-07-20T01:08:49.086868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To iterate over the train data\n",
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(\"[BATCH]\", idx+1)\n",
    "    print(x, y)"
   ],
   "id": "bdf49d5bcfe1469e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH] 1\n",
      "tensor([[ 2.3000, -1.1000],\n",
      "        [-1.2000,  3.1000]]) tensor([1, 0])\n",
      "[BATCH] 2\n",
      "tensor([[-0.9000,  2.9000],\n",
      "        [ 2.7000, -1.5000]]) tensor([0, 1])\n",
      "[BATCH] 3\n",
      "tensor([[-0.5000,  2.6000]]) tensor([0])\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see, there were 5 record in the train data and the five ones are retrieved exactly one time. The problem here is the last batch, which could disturb the convergence during training, so we can just drop the last batch",
   "id": "72a6db2e9c9b027"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T01:13:58.218448Z",
     "start_time": "2025-07-20T01:13:58.211790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2, # number of record retrieved in each iteration\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(\"[BATCH]\", idx+1)\n",
    "    print(x, y)\n"
   ],
   "id": "7b23a37d9707e390",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH] 1\n",
      "tensor([[-0.9000,  2.9000],\n",
      "        [-1.2000,  3.1000]]) tensor([0, 0])\n",
      "[BATCH] 2\n",
      "tensor([[-0.5000,  2.6000],\n",
      "        [ 2.7000, -1.5000]]) tensor([0, 1])\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Another important parameter in the `DataLoader` is the num_workers, when set to 0, the data loading will be in the main process. So, as seen in the figure, in each iteration the CPU has to interrupt the processing of the model, to load the data while the GPU keeps waiting.\n",
    "\n",
    "This bottleneck is solve using multiple workers, where multiple processes are launched to load the data in parallel and leaving the main process to the processing of the model. In this way, the data is already queued up in the background when the model needs it.\n",
    "\n",
    "![num_workers.png](./images/num_workers.png)"
   ],
   "id": "c55dfc12f2e04e44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> IMPORTANT: increase the num_workers in tiny datasets o in Jupyter notebooks can be harmful. In tiny datasets because DataLoader has to start many processes and in Jupyter notebooks because it can generate issues related with the sharing of resources between different processes.\n",
    "\n",
    "Classical choice is num_workers=4"
   ],
   "id": "c1d322147ee0cdaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "\n",
    "Now that we have the neural network defined and the DataLoader ready, we can combine all of this.\n",
    "\n",
    "> Validation dataset: in practice, it is used a third dataset, it is similar to the test dataset. The difference is that validation dataset to tweak hyperparameters and"
   ],
   "id": "3e053435436d6d5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T02:34:14.310516Z",
     "start_time": "2025-07-20T02:34:14.294033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(\n",
    "    num_inputs=2, # because each input has 2 features\n",
    "    num_outputs=2 # because labels in our data are 0 or 1\n",
    ")\n",
    "\n",
    "# stochastic gradient descent with learning rate of 0.5\n",
    "# learning rate is a tunable parameter, we need a value\n",
    "# where the loss converges after some epochs\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "# another tunable parameter\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train() and eval() change the mode of the model, which is\n",
    "    # important for component that works different during\n",
    "    # training and inference, such as dropout or batch normalization,\n",
    "    # as we don't use that, train() and eval() are redundant\n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        logits = model.forward(features)\n",
    "        # loss function + activation function (softmax)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # set gradients to zero to avoid accumulation\n",
    "        optimizer.zero_grad()\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # use the gradients to update the model parameters\n",
    "        # to minimize the loss by multiplying the gradients\n",
    "        # with the learning rate and adding the negative\n",
    "        # result to the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"[EPOCH] {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx+1:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train/Val Loss: {loss:0.2f}\")\n",
    "    model.eval()"
   ],
   "id": "da8dd8bd6289c7c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH] 001/003 | Batch 001/002 | Train/Val Loss: 0.75\n",
      "[EPOCH] 001/003 | Batch 002/002 | Train/Val Loss: 0.65\n",
      "[EPOCH] 002/003 | Batch 001/002 | Train/Val Loss: 0.44\n",
      "[EPOCH] 002/003 | Batch 002/002 | Train/Val Loss: 0.13\n",
      "[EPOCH] 003/003 | Batch 001/002 | Train/Val Loss: 0.03\n",
      "[EPOCH] 003/003 | Batch 002/002 | Train/Val Loss: 0.00\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T03:06:15.257718Z",
     "start_time": "2025-07-20T03:06:15.250906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# now we can make predictions\n",
    "model.eval()\n",
    "\n",
    "torch.set_printoptions(sci_mode=False) # only changes notation\n",
    "with torch.no_grad():\n",
    "    # the output are logits so we apply softmax to get the probabilities\n",
    "    probs = torch.softmax(model(x_train), dim=1)\n",
    "\n",
    "print(probs)\n"
   ],
   "id": "9bba6c95bb057772",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.9991,     0.0009],\n",
      "        [    0.9982,     0.0018],\n",
      "        [    0.9949,     0.0051],\n",
      "        [    0.0491,     0.9509],\n",
      "        [    0.0307,     0.9693]])\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T03:06:19.326122Z",
     "start_time": "2025-07-20T03:06:19.321633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get the indexes of the highest values along rows (rows are dim 1)\n",
    "predictions = torch.argmax(probs, dim=1)\n",
    "print(predictions)"
   ],
   "id": "f0697f18b9a03cf1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T03:12:19.163631Z",
     "start_time": "2025-07-20T03:12:19.157153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the results manually\n",
    "print(predictions == y_train)\n",
    "\n",
    "print(torch.sum(predictions == y_train))"
   ],
   "id": "19087156622c4dd5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True])\n",
      "tensor(5)\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can check the results as above, but we can generalize as below",
   "id": "d4334e311cdec4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T03:20:01.900427Z",
     "start_time": "2025-07-20T03:20:01.894643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "    model = model.eval()\n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "    # process the data in small parts due to memory limitations\n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(features)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        compare = labels == predictions\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "    return (correct / total_examples).item()"
   ],
   "id": "1949e3c9b000368c",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T03:21:37.257828Z",
     "start_time": "2025-07-20T03:21:37.251054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'[TRAIN ACCURACY] {compute_accuracy(model, train_loader) * 100}%')\n",
    "print(f'[TEST ACCURACY] {compute_accuracy(model, test_loader) * 100}%')"
   ],
   "id": "839fed030c4659ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN ACCURACY] 100.0%\n",
      "[TEST ACCURACY] 100.0%\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save and load models",
   "id": "f7dfca9762eb8f88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T05:41:17.673254Z",
     "start_time": "2025-07-20T05:41:17.660674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# to save a model we use a state_dict that maps each layer in the\n",
    "# model its weights and biases\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ],
   "id": "9cfed80861b7e99e",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T05:42:47.930257Z",
     "start_time": "2025-07-20T05:42:47.919537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# to use the model we have to define a network that matches the\n",
    "# original model\n",
    "model = NeuralNetwork(2, 2)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ],
   "id": "d90153745a3246f6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Optimization training with GPU\n",
    "\n",
    "In PyTorch, a device is where computation occur, where data resides. Examples are CPU and GPU."
   ],
   "id": "b55288a44b1ad380"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T06:00:22.506340Z",
     "start_time": "2025-07-20T06:00:22.500392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# with cpu\n",
    "tensor_1 = torch.tensor([1., 2., 3.])\n",
    "tensor_2 = torch.tensor([4., 5., 6.])\n",
    "print(tensor_1 + tensor_2)"
   ],
   "id": "38e11cc92906441",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T06:01:10.364536Z",
     "start_time": "2025-07-20T06:01:10.104439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# with gpu\n",
    "tensor_1 = tensor_1.to(\"cuda\")\n",
    "tensor_2 = tensor_2.to(\"cuda\")\n",
    "print(tensor_1 + tensor_2)"
   ],
   "id": "715fb9230acc60fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we can use the GPU to train our model with little changes",
   "id": "c5c9797b3dae99ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T06:09:07.603473Z",
     "start_time": "2025-07-20T06:09:07.577261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(\n",
    "    num_inputs=2, # because each input has 2 features\n",
    "    num_outputs=2 # because labels in our data are 0 or 1\n",
    ")\n",
    "\n",
    "# NEW: DEVICE AS CUDA -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# NEW: TRANSFER MODEL TO GPU -----------------------\n",
    "model.to(device)\n",
    "\n",
    "# stochastic gradient descent with learning rate of 0.5\n",
    "# learning rate is a tunable parameter, we need a value\n",
    "# where the loss converges after some epochs\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "# another tunable parameter\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train() and eval() change the mode of the model, which is\n",
    "    # important for component that works different during\n",
    "    # training and inference, such as dropout or batch normalization,\n",
    "    # as we don't use that, train() and eval() are redundant\n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "\n",
    "        # NEW: TRANSFER DATA TO GPU ----------------------\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        logits = model.forward(features)\n",
    "        # loss function + activation function (softmax)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # set gradients to zero to avoid accumulation\n",
    "        optimizer.zero_grad()\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # use the gradients to update the model parameters\n",
    "        # to minimize the loss by multiplying the gradients\n",
    "        # with the learning rate and adding the negative\n",
    "        # result to the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"[EPOCH] {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx+1:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train/Val Loss: {loss:0.2f}\")\n",
    "    model.eval()\n"
   ],
   "id": "87d8632070a1a33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH] 001/003 | Batch 001/002 | Train/Val Loss: 0.75\n",
      "[EPOCH] 001/003 | Batch 002/002 | Train/Val Loss: 0.65\n",
      "[EPOCH] 002/003 | Batch 001/002 | Train/Val Loss: 0.44\n",
      "[EPOCH] 002/003 | Batch 002/002 | Train/Val Loss: 0.13\n",
      "[EPOCH] 003/003 | Batch 001/002 | Train/Val Loss: 0.03\n",
      "[EPOCH] 003/003 | Batch 002/002 | Train/Val Loss: 0.00\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The above code runs in only 1 device, in order to reduce the training times even more, we can use multiple GPUs using distributed training.\n",
    "\n",
    "The most basic strategy is DistributedDataParallel (DDP). This works as following:\n",
    "- PyTorch launches a separate process on each GPU\n",
    "- Each process receives a copy of the model\n",
    "- The DataLoader sends a unique minibatch to every GPU\n",
    "- Each model computes different gradients and outputs, to synchronize them, the gradients are averaged and sent to other GPUs to have the same updated weights\n",
    "- In this way, the training time should reduce as the numbers of GPUs increases.\n",
    "> DDP doesn't work in Jupyter notebooks because DDP needs to spawn multiple processes with its own python interpreter instance"
   ],
   "id": "4dbf3915acc2a8b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Code for DDP is in pytorch_one_hour_DDP.py",
   "id": "8a5d0fb3f5fbe7e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Resources:\n",
    "\n",
    "https://sebastianraschka.com/teaching/pytorch-1h/"
   ],
   "id": "c94f67e622050a4c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
