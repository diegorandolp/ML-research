{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PyTorch\n",
    "\n",
    "It is composed by:\n",
    "1. PyTorch tensors = numpy + GPU\n",
    "2. Autograd (automatic differentiation engine) to compute the gradients for tensor operations. Eg: backpropagation.\n",
    "3. Deep learning library that contains pre-trained models, loss functions, etc.\n",
    "\n",
    "We will go through every component"
   ],
   "id": "cdbac2ca0f0bd749"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PyTorch tensors",
   "id": "1600e76ca57e1f0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T16:12:18.173574Z",
     "start_time": "2025-07-19T16:12:17.035110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.__version__\n"
   ],
   "id": "ad5095b07147d2df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0+cu126'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T16:18:03.642335Z",
     "start_time": "2025-07-19T16:18:01.812121Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.is_available())",
   "id": "4688e7e9371a844d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T16:54:32.722241Z",
     "start_time": "2025-07-19T16:54:32.714614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 0D tensor (scalar)\n",
    "tensor0d = torch.tensor(1)\n",
    "print(tensor0d)\n",
    "# 1D tensor (vector)\n",
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "print(tensor1d)\n",
    "# 2D tensor (matrix)\n",
    "tensor2d = torch.tensor([[1, 2], [3, 4]])\n",
    "print(tensor2d)\n",
    "# 3D tensor\n",
    "tensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "print(tensor3d)"
   ],
   "id": "6a868c0348c33bad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T16:55:57.223879Z",
     "start_time": "2025-07-19T16:55:57.219773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# default 64-bit integer\n",
    "print(tensor1d.dtype)"
   ],
   "id": "f63bf91ada160602",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T16:56:50.357403Z",
     "start_time": "2025-07-19T16:56:50.351885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# default 32-bit precision\n",
    "floatvec = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(floatvec.dtype)"
   ],
   "id": "2db56ef9e8aa87c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T16:58:59.641769Z",
     "start_time": "2025-07-19T16:58:59.638965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# change type\n",
    "tensor1d_float = tensor1d.to(torch.float32)\n",
    "print(tensor1d_float.dtype)"
   ],
   "id": "cc467a3e3ded11fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:00:17.100318Z",
     "start_time": "2025-07-19T17:00:17.095530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# shape of a tensor\n",
    "print(tensor0d.shape)\n",
    "print(tensor1d.shape)\n",
    "print(tensor2d.shape)\n",
    "print(tensor3d.shape)"
   ],
   "id": "a4c42fdde2b99c18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:03:03.268705Z",
     "start_time": "2025-07-19T17:03:03.261999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reshape a tensor\n",
    "tensor2d.reshape(4, 1)"
   ],
   "id": "86c5c9614139f202",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:03:27.234043Z",
     "start_time": "2025-07-19T17:03:27.228310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reshape a tensor (common method)\n",
    "tensor2d.view(4, 1)"
   ],
   "id": "85712f799d16ae5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:03:48.136099Z",
     "start_time": "2025-07-19T17:03:48.123321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transpose\n",
    "tensor2d.T"
   ],
   "id": "147e923c57a12c00",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3],\n",
       "        [2, 4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:05:10.770053Z",
     "start_time": "2025-07-19T17:05:10.761116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# matmul 1\n",
    "tensor2d.matmul(tensor2d.T)"
   ],
   "id": "e77970fa87b3f102",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 11],\n",
       "        [11, 25]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:05:40.101808Z",
     "start_time": "2025-07-19T17:05:40.095706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# matmul 2\n",
    "tensor2d @ tensor2d.T"
   ],
   "id": "2b182f1ef034dbf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 11],\n",
       "        [11, 25]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PyTorch autograd engine",
   "id": "86885e521fa1e8a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:16:36.693355Z",
     "start_time": "2025-07-19T17:16:36.681447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Suppose we have a model with the weight w1 and th bias b,\n",
    "# to compute the gradients, pytorch computes a graph in the background\n",
    "# as shown in the following figure\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0]) # true label\n",
    "x1 = torch.tensor([1.1]) # input\n",
    "w1 = torch.tensor([2.2]) # weight\n",
    "b = torch.tensor([0.0]) # bias\n",
    "\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z) # predicted label\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "print(\"[a]\", a)\n",
    "print(\"[y]\", y)\n",
    "print(\"[loss]\", loss)"
   ],
   "id": "ee8a4f578fa27827",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[a] tensor([0.9183])\n",
      "[y] tensor([1.])\n",
      "[loss] tensor(0.0852)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following figure illustrates the graph of the above 'model'.\n",
    "\n",
    "As long as the final node, in this case `loss = L(a,y)` has the requires_grad attribute set to True, pytorch will build the graph to compute the gradients.\n",
    "\n",
    "The way pytorch compute the gradients is from right to left, called backpropagation, it starts from the output layer (loss) and goes backward to the input layer.\n",
    "\n",
    "In this way, pytorch computes the gradient of the loss respect to each parameter (weights and biases) to update these parameters during training.\n",
    "\n",
    "![pytorch_automatic_differentiation.png](./images/pytorch_automatic_differentiation.png)\n"
   ],
   "id": "cd09748e18b4c1b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:55:22.448040Z",
     "start_time": "2025-07-19T17:55:22.435653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# in the previous code the code pytorch didn't build the graph\n",
    "# because there were no terminal nodes with the requires_grad\n",
    "# as True. In this code, the graph is built\n",
    "\n",
    "# This is where the automatic differentiation engine is important,\n",
    "# given the graph, the engine can compute the gradients using the\n",
    "# function grad.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "# by default, the graph is deleted after the gradients are computed\n",
    "# we retain it to use it later\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b = grad(loss, b, retain_graph=True)\n",
    "\n",
    "print(grad_L_w1)\n",
    "print(grad_L_b)"
   ],
   "id": "dc0c694d69e7254f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T17:59:46.786473Z",
     "start_time": "2025-07-19T17:59:46.780413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# anyway, the common way to compute the gradients is using the\n",
    "# method backward, the results will be stored in the grad attribute\n",
    "loss.backward()\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ],
   "id": "2bbe6b911dfc8e4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PyTorch as a deep learning library",
   "id": "5b226567b114091a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Create a MLP with PyTorch, similar to the following image, this network will have:\n",
    "\n",
    "- 50 inputs\n",
    "- 30 neurons in the 1st layer, resulting in:\n",
    "    - 50x30 weights to calculate\n",
    "    - 30 multilinear equations thus 30 biases\n",
    "- 20 neurons in the 2nd layer, resulting in:\n",
    "    - 30x20 weights to calculate\n",
    "    - 20 multilinear equations thus 20 biases\n",
    "- 3 outputs, resulting in:\n",
    "    - 20x3 weights to calculate\n",
    "    - 3 biases\n",
    "\n",
    "Counting all, it gives 2213 parameters to compute\n",
    "\n",
    "![mlp.png](./images/mlp.png)\n",
    "\n"
   ],
   "id": "c696af3f0009a3df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:08:47.739938Z",
     "start_time": "2025-07-19T22:08:47.735805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# our class inherits the Module subclass because it allows us to encapsulate\n",
    "# the layers and operations and track the model's parameters\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    # to define the network layers\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        # calls the Module class constructor\n",
    "        super().__init__()\n",
    "        # encapsulate all the layers\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # output layer\n",
    "            torch.nn.Linear(20, num_outputs)\n",
    "        )\n",
    "    # to define how the input data passes through the network\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ],
   "id": "22ae964321e2479a",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:08:49.334351Z",
     "start_time": "2025-07-19T22:08:49.329624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model = NeuralNetwork(50, 3)\n",
    "print(model)"
   ],
   "id": "3326f4d7965725b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:08:50.761031Z",
     "start_time": "2025-07-19T22:08:50.756402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# same result calculated before\n",
    "num_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad\n",
    ")\n",
    "print('[Parameters]', num_params)"
   ],
   "id": "a7304f11b1afb88f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameters] 2213\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:08:52.187253Z",
     "start_time": "2025-07-19T22:08:52.180726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# As indicated above the trainable parameters have requires_grad\n",
    "# as True, it occurs in the Linear layers, for example, in the\n",
    "# first linear layer which was initialized with randoms:\n",
    "print(model.layers[0].weight)"
   ],
   "id": "67444dbda9ceed56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:08:53.663564Z",
     "start_time": "2025-07-19T22:08:53.658560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(model.layers[0].weight.shape)\n",
    "print(model.layers[0].bias.shape)"
   ],
   "id": "64e07160acd624b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 50])\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:08:57.709103Z",
     "start_time": "2025-07-19T22:08:57.702615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# forward pass without training\n",
    "\n",
    "# tensor of inputs\n",
    "X = torch.rand((1, 50))\n",
    "out = model.forward(X)\n",
    "print(out)"
   ],
   "id": "3582f475031c4475",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1670,  0.1001, -0.1219]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The previous result also gives us the last used function to compute in the graph, pytorch uses this information during backpropagation. It means, `mm` for matrix multiplication followed by `Add` for addition.\n",
    "\n",
    "When we use models only for inferent rather than training, we don't need the creation of the graph, in fact, it would be a waste of resources, so there is a better way in this case:"
   ],
   "id": "58cc4a421aee2655"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:23:08.461307Z",
     "start_time": "2025-07-19T22:23:08.456195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    out = model.forward(X)\n",
    "print(out)"
   ],
   "id": "e8e562dc4e340fec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1670,  0.1001, -0.1219]])\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Common practice**\n",
    "Create models that return `logits` as outputs without an activation function, the `logits` are Real numbers. This happens because pytorch combine the activation function and the loss for efficiency (use cancellation tricks), so the combined functions expect `logits` and output probabilities.\n",
    "Eg:\n",
    "- CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "- BCEWithLogitsLoss = Sigmoid + BCELoss"
   ],
   "id": "3e53073eb1968a5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:42:39.736197Z",
     "start_time": "2025-07-19T22:42:39.728095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# apply the activation function outside the creation of the model\n",
    "# this one in particular ensure all output values are positive and\n",
    "# sum 1\n",
    "with torch.no_grad():\n",
    "    out = torch.softmax(model(X), dim=1)\n",
    "print(out)"
   ],
   "id": "dceb0fb0288a52a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2983, 0.3896, 0.3121]])\n"
     ]
    }
   ],
   "execution_count": 46
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
